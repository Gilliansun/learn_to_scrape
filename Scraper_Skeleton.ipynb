{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MIT Political Science Methods Series  \n",
    "Spring 2018  \n",
    "Andy Halterman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Most news sites and similar web pages can be scraped through a three step process:\n",
    "\n",
    "1. given a link to an article, extracting and formatting all the needed info from the page\n",
    "2. given an archive-type page of links, finding all the links on the page.\n",
    "3. iterating through each page of archives, scraping all the pages from it, and saving to disk\n",
    "\n",
    "We'll write one function for each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Libraries and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# requests is for general HTTP loading\n",
    "import requests\n",
    "# BeautifulSoup is an HTML parser\n",
    "from bs4 import BeautifulSoup\n",
    "# JSON is a nice format for writing out\n",
    "# ujson can handle datetimes better and is a drop in replacement for the json module\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sometimes you'll need the regular expressions library and a date library\n",
    "import re\n",
    "import dateutil.parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Page scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll write a function here that takes in the URL of an article or page, extracts the information we want from HTML, and structures the output.\n",
    "\n",
    "Python things to learn:\n",
    "\n",
    "- calling methods from objects\n",
    "- what a dictionary is\n",
    "- defining a function\n",
    "\n",
    "HTML things to learn: \n",
    "\n",
    "- Chrome inspector\n",
    "- what HTML tags look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# find the URL of an article to scrape\n",
    "url = \"https://reliefweb.int/report/afghanistan/more-200-displaced-families-receive-cash-assistance-laghman-province\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download article page and get content\n",
    "page = \n",
    "content = \n",
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to BeautifulSoup\n",
    "soup = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract body text from page HTML (we'll do this together)\n",
    "body = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract title from page HTML\n",
    "title = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract author from page HTML\n",
    "author = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract date from page HTML\n",
    "raw_date = \n",
    "# convert from raw text into a standardized form\n",
    "date = dateutil.parser.parse(date_raw)\n",
    "# Put it into a standard ISO format \n",
    "date = date.strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# theme\n",
    "theme = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# put it all together!\n",
    "def page_scraper(url):\n",
    "    \"\"\"Function to scrape a page\"\"\"\n",
    "    # Put all your individual pieces from above into here.\n",
    "    title = \"\"\n",
    "    ...\n",
    "    article = {\n",
    "        \"title\" : title,\n",
    "        \"text\" : text,\n",
    "        \"date\" : date,\n",
    "        \"author\" : author,\n",
    "        \"theme\" : theme}\n",
    "    return article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test it\n",
    "url = \"\"\n",
    "page_scraper(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link getter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, we need to get the URLS of all the pages we want to scrape. We can do this by finding the directory pages, where the links are on it, and how to get all available directory pages.\n",
    "\n",
    "Python things to learn here:\n",
    "\n",
    "- getting values from dictionaries\n",
    "- for loops and list comprehensions\n",
    "- regex with the `re` library\n",
    "- basic string operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Function to get all the article links from a single directory page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = \"https://reliefweb.int/country/afg?format=8&page=1\"\n",
    "page = \n",
    "content = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = soup.find(\"?????\").find_all(\"a\")\n",
    "print(links[0:5])\n",
    "\n",
    "# pull out just the links\n",
    "links = [i['href'] for i in links]\n",
    "links[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uh oh! `links` is full of all sorts of garbage. Is there a term\n",
    "#  that we can search for to reliably pull out article links only?\n",
    "links = [i for i in links if bool(re.match(\"?????\", i))]\n",
    "links[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These aren't complete urls! We can use string operations plus a list comprehension to fix this:\n",
    "links = [\"https://reliefweb.int/\" + i for i in links]\n",
    "links[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Put it all together into a function that takes in a \"page number\" \n",
    "#  and returns all the links to scrape from it.\n",
    "def page_to_link(page_num):\n",
    "    # how to use .format()\n",
    "    link = \"https://reliefweb.int/country/afg?format=8&page={0}#content\".format(page_num)\n",
    "    # download the page\n",
    "    # get its content\n",
    "    # soupify\n",
    "    # pull out links\n",
    "    # clean links\n",
    "    return links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it together\n",
    "\n",
    "Now we have a function that'll take a page number for the archive page and return all the links.\n",
    "We have another function that'll take in an article URL and give us the structured content from the page.\n",
    "\n",
    "Let's put them together and download a (small!) range of stories.\n",
    "\n",
    "Note: let's be nice to the UN and not all download the whole thing at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get all the links we want to scrape\n",
    "all_links = []\n",
    "\n",
    "for num in range(1, 5):\n",
    "    lks = page_to_link(num)\n",
    "    all_links.extend(lks) # extend! not append.\n",
    "    \n",
    "len(all_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_content = []\n",
    "\n",
    "for link in all_links[10:20]: # be nice to reliefweb and only get some \n",
    "    try:\n",
    "        content = page_scraper(link)\n",
    "        all_content.append(content) # back to append!\n",
    "    except Exception as e:\n",
    "        # if something goes wrong, keep trucking,\n",
    "        #  but print out the link so we can diagnose it.\n",
    "        print(e)\n",
    "        print(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(all_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_content[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving\n",
    "\n",
    "Now that we have some results as a list of dictionaries, we can store is as a JSON file. JSON and dictionaries are almost equivalent, so it's a natural form to save a dict as a JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FILENAME = \"reliefweb.json\"\n",
    "\n",
    "with open(FILENAME, \"w\") as f:\n",
    "    json.dump(all_content, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read it back in\n",
    "\n",
    "If you want to load it back in later to analyze, you can do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FILENAME = \"reliefweb.json\"\n",
    "\n",
    "with open(FILENAME, \"r\") as f:\n",
    "    loaded_content = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# is it the same?\n",
    "assert loaded_content[4] == all_content[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
